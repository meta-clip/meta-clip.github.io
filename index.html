<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <!-- ‚ú¶ Social banners (fill paths to your exported banners) -->
  <meta name="description" content="Meta CLIP 2 is a worldwide scaling recipe for training CLIP on web-scale multilingual image‚Äìtext pairs, achieving state-of-the-art across English and non-English benchmarks.">
  <meta property="og:title" content="MetaCLIP 2: A Worldwide Scaling Recipe"/>
  <meta property="og:description" content="A simple, generalizable recipe to train CLIP from worldwide web data with mutual benefits across English and non-English, setting new multilingual SOTA."/>
  <meta property="og:url" content="https://your-domain.com/selfcite"/>
  <meta property="og:image" content="static/images/metaclip2.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="MetaCLIP 2: A Worldwide Scaling Recipe"/>
  <meta name="twitter:description" content="Worldwide scaling recipe for CLIP on multilingual web data with new SOTA results."/>
  <meta name="twitter:image" content="static/images/metaclip2.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="CLIP, vision-language, contrastive learning, multilingual, retrieval, zero-shot, Meta CLIP 2, web-scale">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MetaCLIP 2 ¬∑ A Worldwide Scaling Recipe</title>
  <link rel="icon" type="image/png" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Bulma + extras -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!-- MathJax for LaTeX rendering -->
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    svg: { fontCache: 'global' }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hero ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MetaCLIP 2: A Worldwide Scaling Recipe</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://people.csail.mit.edu/yungsung/" target="_blank">Yung-Sung Chuang</a>,</span>
            <span class="author-block"><a href="https://x.com/yangli625" target="_blank">Yang Li</a>,</span>
            <span class="author-block"><a href="https://x.com/dongwang218" target="_blank">Dong Wang</a>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=P7ma7pAAAAAJ&hl=en" target="_blank">Ching-Feng Yeh</a>,</span>
            <span class="author-block"><a href="https://www.linkedin.com/in/kehan-lyu-588217155/" target="_blank">Kehan Lyu</a>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=-loJh1EAAAAJ" target="_blank">Ramya Raghavendra</a>,</span>
            <span class="author-block"><a href="http://people.csail.mit.edu/jrg" target="_blank">James Glass</a>,</span>
            <span class="author-block"><a href="https://www.linkedin.com/in/leo-huang/" target="_blank">Lifei Huang</a>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=lMkTx0EAAAAJ&hl=en" target="_blank">Jason Weston</a>,</span>
            <span class="author-block"><a href="https://www.cs.washington.edu/people/faculty/lsz" target="_blank">Luke Zettlemoyer</a>,</span>
            <span class="author-block"><a href="https://xinleic.xyz/" target="_blank">Xinlei Chen</a>,</span>
            <span class="author-block"><a href="https://liuzhuang13.github.io/" target="_blank">Zhuang Liu</a>,</span>
            <span class="author-block"><a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>,</span>
            <span class="author-block"><a href="https://scottyih.org" target="_blank">Wen-tau Yih</a>,</span>
            <span class="author-block"><a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,</span>
            <span class="author-block"><a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Meta FAIR ¬∑ MIT CSAIL ¬∑ Princeton University ¬∑ New York University<br/>To appear at NeurIPS 2025</span>
          </div>
          <div class="column has-text-centered" style="margin-top:0.75rem;">
            <div class="publication-links">
              <!-- PDF -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2507.22062" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                </a>
              </span>
              <!-- Code -->
              <span class="link-block">
                <a href="https://github.com/facebookresearch/MetaCLIP" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
              <!-- Model -->
              <span class="link-block">
                <a href="https://huggingface.co/models?other=metaclip_2" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">ü§ó</span><span>Models</span>
                </a>
              </span>
              <!-- arXiv -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.22062" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div><!-- column -->
      </div><!-- columns -->
    </div><!-- container -->
  </div><!-- hero-body -->
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Abstract ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-10" style="font-size:1.3rem;">
        <!-- Title & Authors -->
        <!-- Main Content -->
        <div class="content has-text-justified">
          <p>Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting tasks from zero-shot classification and retrieval to serving as encoders for multimodal large language models (MLLMs). Although CLIP has been successfully trained on billion-scale image‚Äìtext pairs from the English web, scaling CLIP further to learn from worldwide web data is challenging: (1) there is no curation method to handle data points from the non-English world; (2) existing multilingual CLIP models underperform their English-only counterparts‚Äîthe common ‚Äúcurse of multilinguality.‚Äù</p>
        </div>
        <figure class="image my-5">
          <img src="static/images/metaclip2.jpg" alt="Meta CLIP 2">
        </figure>
        <div class="content has-text-justified">
          <p><strong>MetaCLIP 2</strong> presents the first recipe to train CLIP from scratch on worldwide web-scale image‚Äìtext pairs. To generalize the findings, we conduct rigorous ablations with minimal changes necessary to address the above challenges and provide a simple recipe that enables <em>mutual benefits</em> between English and non-English data.</p>
          <p>In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and‚Äîwithout system-level confounders such as translation or bespoke architectural changes‚Äîsets new state-of-the-art on multilingual benchmarks, including CVQA (57.4%), Babel-ImageNet (50.2%), and XM3600 (64.3% image-to-text retrieval).</p>
          <p>For details, please see the paper on <a href="https://arxiv.org/abs/2507.22062" target="_blank">arXiv</a>.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Demo Widget ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10" style="font-size:1.3rem;">
        <h2 class="title is-3">Highlights</h2>
        <ul>
          <li>Simple, generalizable data curation for worldwide web-scale image‚Äìtext pairs</li>
          <li>Mutual benefits between English and non-English data without translation</li>
          <li>Zero-shot ImageNet: ViT-H/14 +0.8% over English-only; +0.7% over mSigLIP</li>
          <li>Multilingual SOTA: CVQA 57.4%, Babel-ImageNet 50.2%, XM3600 64.3% (i2t)</li>
        </ul>
      </div>
    </div>
  </div>
  </section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Conclusion ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10" style="font-size:1.3rem;">
        <h2 class="title is-3">Conclusion</h2>
        <p><strong>Meta CLIP 2</strong> demonstrates that scaling CLIP on worldwide web data can be both simple and effective. With minimal yet principled changes for data curation and training, our recipe delivers mutual gains across English and non-English, and achieves new state-of-the-art on multiple multilingual benchmarks while maintaining strong English performance. We release the recipe and code to facilitate reproduction and adoption in future vision‚Äìlanguage research.</p>
      </div>
    </div>
  </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ BibTeX ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10" style="font-size:1.3rem;">
    <h2 class="title is-3">Citation</h2>
    <p>To cite this work, please use the following BibTeX entry:</p>

<pre style="font-size: 0.8rem;">@inproceedings{chuang2025metaclip2,
    title={MetaCLIP 2: A Worldwide Scaling Recipe},
    author={Yung-Sung Chuang and Yang Li and Dong Wang and Ching-Feng Yeh and Kehan Lyu and Ramya Raghavendra and James Glass and Lifei Huang and Jason Weston and Luke Zettlemoyer and Xinlei Chen and Zhuang Liu and Saining Xie and Wen-tau Yih and Shang-Wen Li and Hu Xu},
    booktitle={Advances in Neural Information Processing Systems},
    note={to appear},
    year={2025},
    url={https://arxiv.org/abs/2507.22062},
    eprint={2507.22062},
    archivePrefix={arXiv}
}</pre>

  </div>
    </div>
    </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. Feel free to reuse the source‚Äîjust link back in the footer.<br>
            Licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
